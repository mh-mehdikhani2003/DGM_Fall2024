{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOZpPDTIh-RO"
      },
      "source": [
        "# Import & Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vWzundbcwibK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4816dc4-47a7-40de-d76f-237a573757ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.2)\n",
            "Requirement already satisfied: flashtext<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from hazm) (2.7)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.9.1)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.24.3)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.11)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.5.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (75.1.0)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.0.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.6)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kZ_pLA5ch-EH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable # This package has predefined gradient and derivative functions\n",
        "\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKPX0enHh4tO",
        "outputId": "309987ba-14f2-4501-fb3e-9667242cb66e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available! Training on GPU ...\n"
          ]
        }
      ],
      "source": [
        "# Check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "device =  torch.device('cuda' if train_on_gpu else 'cpu')\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available. Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available! Training on GPU ...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTnaaTZ-m-LO",
        "outputId": "331487cb-bf0e-4ef4-e5db-b44f6d32fcde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCRLSorjiA7u"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofvHKu4H1Mzg"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGufZEgsmxla"
      },
      "source": [
        "## Download dataset from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAA68WdTm1qW",
        "outputId": "d88534aa-7df7-4f0a-88c0-fbdf0a27c358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e-mAJR0OnE6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b87b6e9-6e2b-4905-8d0c-8266629f7c2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat './kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Dataset URL: https://www.kaggle.com/datasets/miladfa7/persian-wikipedia-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading Persian-WikiText-1.txt.zip to /content\n",
            "100% 27.5M/27.5M [00:02<00:00, 21.7MB/s]\n",
            "100% 27.5M/27.5M [00:02<00:00, 11.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download miladfa7/persian-wikipedia-dataset -f Persian-WikiText-1.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sy4t4T4on0B-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925e1838-ca32-4265-b5d1-cf482ac4db9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/miladfa7/persian-wikipedia-dataset\n",
            "License(s): CC0-1.0\n",
            "Persian-WikiText-1.txt.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download miladfa7/persian-wikipedia-dataset -f Persian-WikiText-1.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HaKpHpSFpKBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26adf28a-a13e-4890-ebc6-9f8303d66a24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/alioraji/persian-stop-words\n",
            "License(s): other\n",
            "Downloading Persian_Stop_Words.txt to /content\n",
            "  0% 0.00/45.5k [00:00<?, ?B/s]\n",
            "100% 45.5k/45.5k [00:00<00:00, 1.02MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download alioraji/persian-stop-words -f Persian_Stop_Words.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dp_tEGxnoRyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "699a2ca7-d88b-4e29-ddb2-55e2475d963c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Persian-WikiText-1.txt.zip\n",
            "  inflating: Persian-WikiText-1.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip Persian-WikiText-1.txt.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chn9yFsOoySg"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBolSs9-1Mzj",
        "outputId": "5ade65c4-684a-4852-b39e-eae57e631311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample cleaned text: عنوان مقاله صفحه اصلی templatestyles srcصفحه اصلی عنوان مقاله ویکی پدیا ویکی پدیا کوته نوشت به صورت وپ و WP یک دانشنامه برخط چندزبانه مبتنی بر وب با محتوای آزاد و همکاری باز است که با همکاری افراد داوطلب نوشته می شود و هر کسی که به اینترنت و وب دسترسی داشته باشد می تواند مقالات آن را ببیند و ویرایش کند. نام ویکی پدیا واژه ای ترکیبی است که از واژه های ویکی وبگاه مشارکتی و انسایکلوپدیا Encyclopedia دانشنامه یا دائرةالمعارف گرفته شده است. هدف ویکی پدیا آفرینش و انتشار جهانی یک دانشنامه با محتوای آز\n",
            "Tokens: ['<s>', 'این', 'یک', 'مثال', 'ساده', 'برای', 'توکن', 'ایز', 'کردن', 'است', '.', '</s>']\n",
            "IDs: [2, 1372, 1390, 2690, 3080, 1421, 27971, 4527, 1912, 1362, 5, 3]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "wiki_file_path = \"Persian-WikiText-1.txt\"\n",
        "\n",
        "with open(wiki_file_path, 'r', encoding='utf-8') as file:\n",
        "    persian_text = file.read()\n",
        "\n",
        "\n",
        "import re\n",
        "def clean_text(text):\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s،؟!.]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "cleaned_text = clean_text(persian_text)\n",
        "\n",
        "print(\"Sample cleaned text:\", cleaned_text[:500])\n",
        "\n",
        "# Tokenization using Byte-Pair Encoding (BPE)\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "\n",
        "trainer = trainers.BpeTrainer(vocab_size=30000, special_tokens=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\"])\n",
        "\n",
        "\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "with open(\"cleaned_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(cleaned_text)\n",
        "\n",
        "tokenizer.train([\"cleaned_text.txt\"], trainer)\n",
        "\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"<s> $A </s>\",\n",
        "    pair=\"<s> $A </s> </s> $B </s>\",\n",
        "    special_tokens=[\n",
        "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    ],\n",
        ")\n",
        "\n",
        "tokenizer.save(\"persian_tokenizer.json\")\n",
        "\n",
        "example_text = \"این یک مثال ساده برای توکنایز کردن است.\"\n",
        "encoded = tokenizer.encode(example_text)\n",
        "\n",
        "print(\"Tokens:\", encoded.tokens)\n",
        "print(\"IDs:\", encoded.ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ". کیفیت داده‌های ورودی مدل:\n",
        "\n",
        "داده‌های خام معمولاً شامل نویز (مانند کاراکترهای غیرضروری، فاصله‌های اضافی، یا کلمات غیرمرتبط) است. وجود نویز در داده‌ها می‌تواند به مشکلات زیر منجر شود:\n",
        "\n",
        "    کاهش دقت مدل.\n",
        "    افزایش زمان آموزش، زیرا مدل باید نویز را نیز پردازش کند.\n",
        "    یادگیری الگوهای نادرست از داده‌ها.\n",
        "\n",
        "به همین دلیل، حذف نویز و تمیز کردن داده‌ها تضمین می‌کند که مدل تنها اطلاعات مرتبط و مهم را پردازش می‌کند.\n",
        "2. کاهش پیچیدگی محاسباتی:\n",
        "\n",
        "    داده‌های تمیز و پیش‌پردازش‌شده اندازه‌ی کوچک‌تر و کارآمدتری دارند.\n",
        "    این امر باعث کاهش مصرف حافظه و زمان پردازش می‌شود و آموزش مدل سریع‌تر و بهینه‌تر انجام می‌گیرد.\n",
        "\n",
        "3. تناسب داده‌ها با مدل:\n",
        "\n",
        "مدل‌های زبانی مانند RNN برای پردازش داده‌های ساختاریافته و استاندارد طراحی شده‌اند. اگر داده‌ها شامل کاراکترهای خاص یا قالب‌های نامتعارف باشند، ممکن است مدل نتواند آن‌ها را به درستی تفسیر کند. بنابراین، تبدیل داده‌ها به فرمت مناسب (مانند توکنایز کردن) برای بهبود عملکرد مدل ضروری است.\n",
        "4. اثرگذاری نویز بر پیش‌بینی مدل:\n",
        "\n",
        "    اگر نویز در داده باقی بماند، مدل ممکن است آن را به‌عنوان الگوهای معنی‌دار یاد بگیرد.\n",
        "    این موضوع باعث می‌شود خروجی مدل قابل اطمینان نباشد و مدل به جای یادگیری مفاهیم مهم، روی جزئیات غیرضروری متمرکز شود."
      ],
      "metadata": {
        "id": "azkbmHX65b2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Byte Pair Encoding (BPE)**\n",
        "\n",
        "یک تکنیک برای توکنایز کردن متن است که کاراکترها و زیرواژه‌ها را برای کاهش اندازه داده‌ها ترکیب می‌کند.  \n",
        "\n",
        "---\n",
        "\n",
        "### **مراحل عملکرد:**\n",
        "1. **شروع با کاراکترهای منفرد:** متن به کوچک‌ترین واحدها (کاراکترها) شکسته می‌شود.\n",
        "2. **ادغام جفت‌های پرتکرار:** جفت‌هایی که بیشترین تکرار دارند، به توکن‌های جدید تبدیل می‌شوند.\n",
        "3. **تکرار فرآیند:** ادغام‌ها ادامه پیدا می‌کنند تا تعداد مشخصی از توکن‌ها ساخته شود.\n",
        "4. **ساخت دیکشنری:** لیستی از توکن‌های پرتکرار ایجاد می‌شود که برای توکنایز کردن متن‌های جدید استفاده می‌شود.\n",
        "\n",
        "---\n",
        "\n",
        "### **مزایا:**\n",
        "- **مدیریت کلمات ناشناخته:** کلمات جدید به زیرواژه‌ها شکسته می‌شوند.\n",
        "- **کاهش طول داده‌ها:** کلمات پرتکرار به توکن‌های واحد تبدیل می‌شوند.\n",
        "- **انعطاف‌پذیری:** در زبان‌های پیچیده مانند فارسی به‌خوبی کار می‌کند.\n",
        "\n",
        "### **معایب:**\n",
        "- **پیچیدگی زمانی:** یادگیری دیکشنری زمان‌بر است.\n",
        "- **شکستن معنایی:** ممکن است برخی کلمات معنای کامل خود را از دست بدهند.\n",
        "- **افزایش طول توکن‌ها:** در کلمات ناشناخته، تعداد توکن‌ها بیشتر می‌شود.\n",
        "\n",
        "---\n",
        "\n",
        "### **کاربردها:**\n",
        "- مدل‌های زبانی پیشرفته (مانند GPT و BERT).\n",
        "- ترجمه ماشینی و پردازش متن چندزبانه.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZawvVUN85dFk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxxI2q1_uQzf"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA Available: \", torch.cuda.is_available())\n",
        "print(\"Device Name: \", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEEI4e5eACyU",
        "outputId": "939a241c-961a-4eed-cc8a-e20c261d406b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available:  True\n",
            "Device Name:  Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "8Pm-75BP1Mzj",
        "outputId": "67355238-abd6-4823-e603-bb76cfb17d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1/10, Loss: 9.2845\n",
            "Epoch 2/10, Loss: 6.8018\n",
            "Epoch 3/10, Loss: 6.4637\n",
            "Epoch 4/10, Loss: 6.1813\n",
            "Epoch 5/10, Loss: 5.8884\n",
            "Epoch 6/10, Loss: 5.5740\n",
            "Epoch 7/10, Loss: 5.2374\n",
            "Epoch 8/10, Loss: 4.8872\n",
            "Epoch 9/10, Loss: 4.5369\n",
            "Epoch 10/10, Loss: 4.1869\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "with open(\"Persian-WikiText-1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "tokens = text.split()\n",
        "vocab = sorted(set(tokens))\n",
        "token_to_idx = {token: idx for idx, token in enumerate(vocab)}\n",
        "idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n",
        "\n",
        "\n",
        "data_indices = [token_to_idx[token] for token in tokens]\n",
        "\n",
        "\n",
        "LIMIT = 10000  # Adjust the limit to your needs\n",
        "data_indices = data_indices[:LIMIT]\n",
        "\n",
        "\n",
        "class NGramDataset(Dataset):\n",
        "    def __init__(self, data, context_size):\n",
        "        self.data = data\n",
        "        self.context_size = context_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.context_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context = self.data[idx:idx + self.context_size]\n",
        "        target = self.data[idx + self.context_size]\n",
        "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "# Hyperparameters\n",
        "context_size = 5\n",
        "embedding_dim = 64\n",
        "hidden_dim = 128\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "dataset = NGramDataset(data_indices, context_size)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class NGramRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, context_size):\n",
        "        super(NGramRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        rnn_out, _ = self.rnn(embedded)\n",
        "        output = self.fc(rnn_out[:, -1, :])\n",
        "        return output\n",
        "\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "model = NGramRNN(vocab_size, embedding_dim, hidden_dim, context_size).to(device)  # Move model to GPU\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for contexts, targets in dataloader:\n",
        "        contexts, targets = contexts.to(device), targets.to(device)  # Move data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(contexts)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    epoch_loss /= len(dataloader)\n",
        "    losses.append(epoch_loss)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "\n",
        "plt.plot(range(1, num_epochs + 1), losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Curve')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"ngram_rnn_model.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#context_size = 5\n",
        "#embedding_dim = 64\n",
        "#hidden_dim = 128\n",
        "#batch_size = 64\n",
        "#num_epochs = 10\n",
        "#learning_rate = 0.001"
      ],
      "metadata": {
        "id": "Wf_a9YsakTYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perplexity:"
      ],
      "metadata": {
        "id": "R2-3LG3GBrf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "def calculate_perplexity(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for contexts, targets in dataloader:\n",
        "            contexts, targets = contexts.to(device), targets.to(device)\n",
        "\n",
        "\n",
        "            outputs = model(contexts)\n",
        "\n",
        "\n",
        "            loss = nn.CrossEntropyLoss()(outputs, targets)\n",
        "            total_loss += loss.item() * contexts.size(0)\n",
        "            total_tokens += contexts.size(0) * contexts.size(1)\n",
        "\n",
        "\n",
        "    avg_log_likelihood = total_loss / total_tokens\n",
        "\n",
        "    perplexity = math.exp(avg_log_likelihood)\n",
        "    return perplexity\n",
        "\n",
        "perplexity = calculate_perplexity(model, dataloader, device)\n",
        "print(f\"Perplexity: {perplexity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efxYjqCqB47j",
        "outputId": "278326a8-a349-4d1e-d2f8-62686897ee19"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 2.1148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "model = NGramRNN(vocab_size, embedding_dim, hidden_dim, context_size).to('cuda')\n",
        "model.load_state_dict(torch.load(\"ngram_rnn_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def custom_tokenizer(prompt):\n",
        "    return prompt.split()\n",
        "\n",
        "\n",
        "def generate_text(model, prompt, max_length=50, device='cuda'):\n",
        "    model.eval()\n",
        "    input_tokens = custom_tokenizer(prompt)\n",
        "    input_ids = torch.tensor([token_to_idx[token] for token in input_tokens], dtype=torch.long).unsqueeze(0).to(device)  # Encode prompt as tensor\n",
        "\n",
        "    generated_text = prompt\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs\n",
        "            next_token_id = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "            next_token = idx_to_token[next_token_id]\n",
        "            generated_text += \" \" + next_token\n",
        "\n",
        "\n",
        "            input_ids = torch.cat([input_ids, torch.tensor([[next_token_id]], device=device)], dim=1)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "prompt = \"امروز روز خوبی است\"\n",
        "generated_text = generate_text(model, prompt, max_length=10, device='cuda')\n",
        "print(\"Generated Text: \", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siREc8OACTLg",
        "outputId": "21614131-97d8-4949-9932-6e4b447c4c90"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-c6fac7571911>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"ngram_rnn_model.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:  امروز روز خوبی است که در سال ۲۰۰۴ به کار می شود. در سال\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformers"
      ],
      "metadata": {
        "id": "7qZXid2CVume"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers tokenizers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3SJV8sXJois",
        "outputId": "e598bbee-b928-4c2d-a697-7acb4b7bd9dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare the Dataset for Training"
      ],
      "metadata": {
        "id": "A31W9NKIbD9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class PersianTextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            self.text = file.read().split('.')\n",
        "\n",
        "\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(sentence).ids for sentence in self.text if sentence.strip()\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded_text = self.encoded_texts[idx]\n",
        "        input_ids = encoded_text[:self.max_length]\n",
        "        padding_length = self.max_length - len(input_ids)\n",
        "\n",
        "\n",
        "        input_ids += [self.tokenizer.token_to_id(\"<pad>\")] * padding_length\n",
        "\n",
        "\n",
        "        attention_mask = [1 if i != self.tokenizer.token_to_id(\"<pad>\") else 0 for i in input_ids]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "dataset = PersianTextDataset(tokenizer, \"cleaned_text.txt\", max_length=128)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Ul060X-66w-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Transformer Model"
      ],
      "metadata": {
        "id": "SKlo0grObvZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len=128):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
        "\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "\n",
        "        embedded = self.embedding(input_ids) + self.positional_encoding[:, :input_ids.size(1), :]\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "\n",
        "        embedded = embedded.permute(1, 0, 2)  # [batch_size, seq_len, d_model] -> [seq_len, batch_size, d_model]\n",
        "        mask = attention_mask == 0  # Mask for padding tokens\n",
        "        transformer_output = self.transformer_encoder(embedded, src_key_padding_mask=mask)\n",
        "\n",
        "\n",
        "        output = self.fc_out(transformer_output.permute(1, 0, 2))  # Back to [batch_size, seq_len, vocab_size]\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "3z-ODbnfbwdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the Model"
      ],
      "metadata": {
        "id": "39-BlEMIb1th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "debug_size = int(len(dataset) * 0.1)  # Use 10% of the dataset for debugging\n",
        "debug_indices = list(range(debug_size))\n",
        "debug_dataloader = torch.utils.data.DataLoader(\n",
        "    Subset(dataset, debug_indices), batch_size=64, shuffle=True\n",
        ")\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "d_model = 256\n",
        "nhead = 8\n",
        "num_layers = 4\n",
        "\n",
        "\n",
        "model = Transformer(vocab_size, d_model, nhead, num_layers)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=5e-5)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id(\"<pad>\"))\n",
        "\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in debug_dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            logits = outputs.view(-1, vocab_size)\n",
        "            targets = input_ids.view(-1)\n",
        "\n",
        "\n",
        "            loss = loss_fn(logits, targets)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(debug_dataloader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWWgwjYDfux2",
        "outputId": "7c7b1c0f-cb5a-4331-e86b-8a9013cd71c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "<ipython-input-15-9defac12ba14>:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-15-9defac12ba14>:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Mixed precision forward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 4.737224984169006\n",
            "Epoch 2/10, Loss: 2.2871448152501817\n",
            "Epoch 3/10, Loss: 1.4500436161605406\n",
            "Epoch 4/10, Loss: 1.0141906413393962\n",
            "Epoch 5/10, Loss: 0.7455249874524668\n",
            "Epoch 6/10, Loss: 0.5643928113537775\n",
            "Epoch 7/10, Loss: 0.43485227840047486\n",
            "Epoch 8/10, Loss: 0.33793307672923717\n",
            "Epoch 9/10, Loss: 0.26414077752073045\n",
            "Epoch 10/10, Loss: 0.20745577879354987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The loss has decreased"
      ],
      "metadata": {
        "id": "qGxcqPTffag_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Text and save model"
      ],
      "metadata": {
        "id": "ARy_sry6cpu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.save(model.state_dict(), \"persian_transformer.pth\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "example_input = tokenizer.encode(\" این یک نمونه تست است. حال\").ids\n",
        "example_input = torch.tensor(example_input).unsqueeze(0).to(device)\n",
        "output = model(example_input, attention_mask=torch.ones_like(example_input).to(device))\n",
        "predicted_ids = output.argmax(-1).squeeze().tolist()\n",
        "predicted_text = tokenizer.decode(predicted_ids)\n",
        "\n",
        "print(\"Generated text:\", predicted_text)\n"
      ],
      "metadata": {
        "id": "HlebW91ncr5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df8c2e9-f177-4fe2-d019-97359aea35c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: این یک نمونه تست است بروز حال\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculate Perplexity"
      ],
      "metadata": {
        "id": "F69x06Z8ZD7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "model = Transformer(vocab_size, d_model, nhead, num_layers)\n",
        "model.load_state_dict(torch.load(\"persian_transformer.pth\"))\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id(\"<pad>\"))\n",
        "\n",
        "# Create a DataLoader for the validation data (if you have a validation dataset)\n",
        "# If you don't have one, you can use a subset of the training dataset for evaluation\n",
        "validation_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "total_loss = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for batch in validation_dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        logits = outputs.view(-1, vocab_size)\n",
        "        targets = input_ids.view(-1)\n",
        "\n",
        "\n",
        "        loss = loss_fn(logits, targets)\n",
        "\n",
        "        total_loss += loss.item() * input_ids.size(0)\n",
        "        total_samples += input_ids.size(0)\n",
        "\n",
        "\n",
        "avg_loss = total_loss / total_samples\n",
        "perplexity = math.exp(avg_loss)\n",
        "\n",
        "print(f\"Perplexity: {perplexity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybs6AxtFZCns",
        "outputId": "cfd029a3-0ac0-4baf-ca5e-4e2e692f26ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "<ipython-input-17-cb7f6bed71e2>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"persian_transformer.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 1.4010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The perplexity has decreased"
      ],
      "metadata": {
        "id": "190cSrdNgyvE"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}